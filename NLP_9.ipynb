{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNoh69Pdq1vx97BTS6LC3oY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShubhamVermaDev9/Natural_Language_Processing-/blob/main/NLP_9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Natural Language Processing â€” 9 Key Techniques Explained**\n",
        "Natural language processing (NLP) is a branch of artificial intelligence that  helps computers comprehend and interact with human language."
      ],
      "metadata": {
        "id": "PCUXBiLTwbBB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1.Tokenization**:Split text\n"
      ],
      "metadata": {
        "id": "h7Z3-1trwhgJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5ajDwdjwVNR",
        "outputId": "eb0ada14-772e-429d-ae8d-51a1f28188bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLTK Tokens: ['Tokenization', 'is', 'a', 'key', 'process', 'in', 'NLP', '.', 'Let', \"'s\", 'see', 'how', 'it', \"'s\", '#', 'working', '.']\n",
            "spaCy Tokens: ['Tokenization', 'is', 'a', 'key', 'process', 'in', 'NLP', '.', 'Let', \"'s\", 'see', 'how', 'it', \"'s\", '#', 'working', '.']\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import spacy\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "text = \"Tokenization is a key process in NLP. Let's see how it's #working.\"\n",
        "nltk_tokens = word_tokenize(text)\n",
        "spacy_sent = nlp(text)\n",
        "spacy_tokens = [token.text for token in spacy_sent]\n",
        "\n",
        "print(\"NLTK Tokens:\", nltk_tokens)\n",
        "print(\"spaCy Tokens:\", spacy_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Part-of-Speech (POS) Tagging**:Grammar labeling\n"
      ],
      "metadata": {
        "id": "WPXWj-ybwvgM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "sentence = \"John likes to watch action movies. I like playing soccer\"\n",
        "tokens = word_tokenize(sentence)\n",
        "pos_tags = nltk.pos_tag(tokens)\n",
        "print(pos_tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67niz-CTwwUC",
        "outputId": "5868620d-560f-4024-e683-fe781e615097"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('John', 'NNP'), ('likes', 'VBZ'), ('to', 'TO'), ('watch', 'VB'), ('action', 'NN'), ('movies', 'NNS'), ('.', '.'), ('I', 'PRP'), ('like', 'IN'), ('playing', 'VBG'), ('soccer', 'NN')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Stemming and Lemmatization**:Normalize words\n"
      ],
      "metadata": {
        "id": "dSginckIxGkO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.stem import LancasterStemmer\n",
        "\n",
        "text = \"Lemmatization important language techniques. \" \\\n",
        "       \"Caring testable eaten was beautifully better driver\"\n",
        "tokens = word_tokenize(text)\n",
        "porter = PorterStemmer()\n",
        "snowball = SnowballStemmer(language='english')\n",
        "lancaster = LancasterStemmer()\n",
        "\n",
        "print(\"{0:16}{1:16}{2:18}{3:18}\".\n",
        "      format(\"Word\",\"Porter\",\"Snowball\",\"Lancaster\"))\n",
        "for w in tokens:\n",
        "    print(\"{0:16}{1:16}{2:18}{3:18}\".\n",
        "          format(w,porter.stem(w),snowball.stem(w),lancaster.stem(w)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D56GRVwHxE-7",
        "outputId": "18752326-a5ae-4f1f-b560-2cc679fe9202"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word            Porter          Snowball          Lancaster         \n",
            "Lemmatization   lemmat          lemmat            lem               \n",
            "important       import          import            import            \n",
            "language        languag         languag           langu             \n",
            "techniques      techniqu        techniqu          techn             \n",
            ".               .               .                 .                 \n",
            "Caring          care            care              car               \n",
            "testable        testabl         testabl           test              \n",
            "eaten           eaten           eaten             eat               \n",
            "was             wa              was               was               \n",
            "beautifully     beauti          beauti            beauty            \n",
            "better          better          better            bet               \n",
            "driver          driver          driver            driv              \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. Stop Words**:Normalize words\n"
      ],
      "metadata": {
        "id": "ljkKAO4QxXAj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "\n",
        "nltk.download('stopwords')\n",
        "text = \"How did you collect those books in Eugene?\"\n",
        "tokens = word_tokenize(text)\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "\n",
        "filtered_tokens = [w for w in tokens if w.lower() not in stop_words]\n",
        "print(f\"Original Tokens: {tokens}\")\n",
        "print(f\"After Removing Stop Words: {filtered_tokens}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czIYWJ9wxbZB",
        "outputId": "38e95ed1-bd35-46d1-f6dc-c70b724f0738"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Tokens: ['How', 'did', 'you', 'collect', 'those', 'books', 'in', 'Eugene', '?']\n",
            "After Removing Stop Words: ['collect', 'books', 'Eugene', '?']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. Bag of Words (BoW) with Count Vector**:Word frequency\n"
      ],
      "metadata": {
        "id": "VmzWTji3xkhZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "\n",
        "text_data = [\n",
        "    \"It was the best of times\",\n",
        "    \"it was the worst of times\",\n",
        "    \"it was the age of wisdom\",\n",
        "    \"it was the age of foolishness was\"\n",
        "]\n",
        "\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "bow_matrix = vectorizer.fit_transform(text_data)\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "bow_array = bow_matrix.toarray()\n",
        "print(\"Vocabulary:\\n\", feature_names)\n",
        "print(\"BoW Document Term Matrix:\\n\", bow_array)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTm1Pk6zxkJ9",
        "outputId": "cee98a92-4105-44ce-aee9-faa1cacea94f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary:\n",
            " ['age' 'best' 'foolishness' 'it' 'of' 'the' 'times' 'was' 'wisdom' 'worst']\n",
            "BoW Document Term Matrix:\n",
            " [[0 1 0 1 1 1 1 1 0 0]\n",
            " [0 0 0 1 1 1 1 1 0 1]\n",
            " [1 0 0 1 1 1 0 1 1 0]\n",
            " [1 0 1 1 1 1 0 2 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wITd1GfkxaDG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **6. TF-IDF**:Word importance\n"
      ],
      "metadata": {
        "id": "30ktEaU8xrEJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "documents = [\n",
        "    'This is the first document.',\n",
        "    'This document is the second document.',\n",
        "    'And this is the third third third one.',\n",
        "    'Is this the first document?',\n",
        "]\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "\n",
        "tfidf_vectors = vectorizer.fit_transform(documents)\n",
        "words = vectorizer.get_feature_names_out()\n",
        "tfidf_scores = {}\n",
        "for i, doc in enumerate(documents):\n",
        "    feature_index = tfidf_vectors[i, :].nonzero()[1]\n",
        "    tfidf_doc = zip([words[idx] for idx in feature_index],\n",
        "                    [tfidf_vectors[i, x] for x in feature_index])\n",
        "    tfidf_scores[i] = {word: score for word, score in tfidf_doc}\n",
        "for doc_id, scores in tfidf_scores.items():\n",
        "    print(f\"Document {doc_id + 1}:\")\n",
        "    for word, score in scores.items():\n",
        "        print(f\"\\t{word}: {score:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lfLd_Tv0xyQ0",
        "outputId": "6854336a-9cf0-4b88-9114-d89fed181b06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document 1:\n",
            "\tthis: 0.3841\n",
            "\tis: 0.3841\n",
            "\tthe: 0.3841\n",
            "\tfirst: 0.5803\n",
            "\tdocument: 0.4698\n",
            "Document 2:\n",
            "\tthis: 0.2811\n",
            "\tis: 0.2811\n",
            "\tthe: 0.2811\n",
            "\tdocument: 0.6876\n",
            "\tsecond: 0.5386\n",
            "Document 3:\n",
            "\tthis: 0.1518\n",
            "\tis: 0.1518\n",
            "\tthe: 0.1518\n",
            "\tand: 0.2909\n",
            "\tthird: 0.8727\n",
            "\tone: 0.2909\n",
            "Document 4:\n",
            "\tthis: 0.3841\n",
            "\tis: 0.3841\n",
            "\tthe: 0.3841\n",
            "\tfirst: 0.5803\n",
            "\tdocument: 0.4698\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **7. N-grams**:Capture context\n"
      ],
      "metadata": {
        "id": "QInQWB6fx2uf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.util import ngrams\n",
        "from nltk.tokenize import word_tokenize\n",
        "text = \"I love reading books about human history.\"\n",
        "tokens = word_tokenize(text)\n",
        "N = 2\n",
        "word_ngrams = list(ngrams(tokens, N))\n",
        "print(f\"{N}-grams:\")\n",
        "for gram in word_ngrams:\n",
        "    print(gram)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xTLXZ9Cax-OM",
        "outputId": "29261176-baf3-44d4-923c-69e4aad29b58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2-grams:\n",
            "('I', 'love')\n",
            "('love', 'reading')\n",
            "('reading', 'books')\n",
            "('books', 'about')\n",
            "('about', 'human')\n",
            "('human', 'history')\n",
            "('history', '.')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **8. Word Embeddings**:Capture context\n"
      ],
      "metadata": {
        "id": "_VgMuCS5yEu3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim\n",
        "from gensim.models import Word2Vec\n",
        "import nltk\n",
        "\n",
        "nltk.download('brown')\n",
        "from nltk.corpus import brown\n",
        "\n",
        "sents = brown.sents()\n",
        "\n",
        "model = Word2Vec(sentences=sents, vector_size=100, window=3, min_count=1)\n",
        "print(model)\n",
        "word_vectors = model.wv\n",
        "output = word_vectors.most_similar(positive=['woman', 'king'],\n",
        "                                   negative=['man'], topn=3)\n",
        "print(f\"king - man + woman = {output}\")\n",
        "\n",
        "print(f\"similarity('king', 'queen'):{model.wv.similarity('king','queen')}\")\n",
        "print(f\"similarity('best', 'worst'):{model.wv.similarity('best','worst')}\")\n",
        "print(f\"similarity('best', 'was'): {model.wv.similarity('best', 'was')}\")\n",
        "similar_words = word_vectors.most_similar(\"Paris\", topn=3)\n",
        "\n",
        "print(f\"Words similar to '{word}': {similar_words}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yueU2GfjyKod",
        "outputId": "d17c2fa8-a8ed-4ad7-e1ec-485a0f1eaeaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.12/dist-packages (4.4.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word2Vec<vocab=56057, vector_size=100, alpha=0.025>\n",
            "king - man + woman = [('heroes', 0.9279902577400208), ('throats', 0.924342930316925), ('inventor', 0.9220629334449768)]\n",
            "similarity('king', 'queen'):0.9242252111434937\n",
            "similarity('best', 'worst'):0.7875627279281616\n",
            "similarity('best', 'was'): 0.38989323377609253\n",
            "Words similar to 'document': [('London', 0.984093427658081), ('Rome', 0.9817825555801392), ('Boston', 0.9792966842651367)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **9. Named Entity Recognition (NER)**:Extract entities\n"
      ],
      "metadata": {
        "id": "hNQVc5YzydpH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "NER = spacy.load(\"en_core_web_sm\")\n",
        "text = \"Apple is a technology company based in Cupertino, California, \" \\\n",
        "       \"founded by Steve Jobs 48 years ago.\"\n",
        "text2 = \"Daniel McDonald's son went to McDonald's and ordered a Happy Meal\"\n",
        "text3 = \"hi my name is shubham verma\"\n",
        "doc = NER(text)\n",
        "named_entities = [(entity.text, entity.label_) for entity in doc.ents]\n",
        "\n",
        "for entity, label in named_entities:\n",
        "    print(f\"Entity: {entity}, Label: {label} ({spacy.explain(label)})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ceagBYe_yjwy",
        "outputId": "719ebf29-9943-498b-c7ad-c09816021358"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entity: Apple, Label: ORG (Companies, agencies, institutions, etc.)\n",
            "Entity: Cupertino, Label: GPE (Countries, cities, states)\n",
            "Entity: California, Label: GPE (Countries, cities, states)\n",
            "Entity: Steve Jobs, Label: PERSON (People, including fictional)\n",
            "Entity: 48 years ago, Label: DATE (Absolute or relative dates or periods)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tzzKoL1H23lx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mOKZXQPU4uQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "T06ljyw_4twT"
      }
    }
  ]
}